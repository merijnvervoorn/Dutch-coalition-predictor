{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###code to fetch documents from PDF reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "\n",
    "# Train the Top2Vec model\n",
    "print(\"Training Top2Vec model. This may take a while...\")\n",
    "model = Top2Vec(documents, speed=\"learn\", workers=4)  # Adjust workers based on your system\n",
    "print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of topics\n",
    "num_topics = model.get_num_topics()\n",
    "print(f\"The model identified {num_topics} topics.\")\n",
    "\n",
    "# Get topic sizes and labels\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "\n",
    "# Display top words for each topic\n",
    "for topic_num in topic_nums:\n",
    "    # Use get_topics() to get all topic words and their scores\n",
    "    topics = model.get_topics()\n",
    "    \n",
    "    # Access the words and scores for the specific topic\n",
    "    topic_words = topics[0][topic_num]  # words for the specific topic\n",
    "    topic_scores = topics[1][topic_num]  # scores for the specific topic\n",
    "    \n",
    "    # Print the top 10 words and their corresponding scores\n",
    "    print(f\"\\nTopic {topic_num}:\")\n",
    "    print(\", \".join(topic_words[:35]))  # Top 10 words\n",
    "    print(\", \".join([f\"{score:.4f}\" for score in topic_scores[:10]]))  # Top 10 word scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### VISUALIZATION OF THE TOPICS AND THEIR PROXIMITY ####\n",
    "\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get document embeddings and topic vectors\n",
    "document_embeddings = model.document_vectors  # or model.document_embeddings\n",
    "topic_embeddings = model.topic_vectors\n",
    "\n",
    "# Reduce dimensions for visualization using UMAP\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='correlation')\n",
    "umap_embeddings = umap_model.fit_transform(document_embeddings)\n",
    "\n",
    "# Plot the documents\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], alpha=0.5, c='blue', s=10)\n",
    "plt.title(\"Document Embeddings\")\n",
    "plt.show()\n",
    "\n",
    "# Reduce dimensions for topic embeddings\n",
    "umap_topic_embeddings = umap_model.fit_transform(topic_embeddings)\n",
    "\n",
    "# Plot the topics\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(umap_topic_embeddings[:, 0], umap_topic_embeddings[:, 1], alpha=0.5, c='red', s=100)\n",
    "plt.title(\"Topic Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display a word cloud for each topic\n",
    "for topic_num in range(model.get_num_topics()):\n",
    "    model.generate_topic_wordcloud(topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####SENTIMENT ANALYSIS POTENTIAL CODE (I suggest diving per political party at first for rough idea)#####\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Step 1: Loop through the first 5 topics\n",
    "for target_topic in range(5):  # Loop through topic 0 to topic 4\n",
    "    print(f\"\\nProcessing Topic {target_topic}...\")\n",
    "\n",
    "    # Step 2: Retrieve documents for the current topic\n",
    "    documents_for_target_topic = []\n",
    "    for i, topic in enumerate(topic_assignments):\n",
    "        if topic == target_topic:\n",
    "            documents_for_target_topic.append(model.documents[i])  # Access the documents from the model\n",
    "    \n",
    "    # Step 3: Print the number of documents associated with this topic\n",
    "    num_documents = len(documents_for_target_topic)\n",
    "    print(f\"Number of documents in Topic {target_topic}: {num_documents}\")\n",
    "\n",
    "    # Step 4: Perform sentiment analysis on the documents for the current topic\n",
    "    sentiment_scores = []\n",
    "    for doc in documents_for_target_topic:\n",
    "        sentiment = analyzer.polarity_scores(doc)  # Get sentiment score\n",
    "        sentiment_scores.append(sentiment['compound'])  # Use the compound score (range from -1 to 1)\n",
    "\n",
    "    # Step 5: Display the sentiment scores for the first few documents\n",
    "    print(f\"\\nSentiment scores for documents in Topic {target_topic}:\")\n",
    "    for i, score in enumerate(sentiment_scores[:5]):  # Display sentiment scores for the first 5 documents\n",
    "        print(f\"Document {i+1}: Sentiment Score = {score}\")\n",
    "\n",
    "    # Step 6: Plot the sentiment distribution for the current topic\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(sentiment_scores, bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Sentiment Distribution for Documents in Topic {target_topic}\")\n",
    "    plt.xlabel(\"Sentiment Score (compound)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
