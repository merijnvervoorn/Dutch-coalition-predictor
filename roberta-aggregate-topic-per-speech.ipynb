{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16137cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new csv file with filename, date, speaker, party, text, scores columns\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b5884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1: 0000698e-a0d5-4007-bd38-38285db4f804.pdf.txt\n",
      "Processing file 2: 00026177-5fb6-41c7-8058-edb4274858d2.pdf.txt\n",
      "Processing file 3: 00026fc1-d112-4e28-b1d6-52a23efe367a.pdf.txt\n",
      "Processing file 4: 000299a9-e3b9-4997-9a25-4e2b00e61dca.pdf.txt\n",
      "Processing file 5: 00035a28-9a80-448e-86c3-9886aa73a9b8.pdf.txt\n",
      "Processing file 6: 000886c8-c66e-49c1-95fc-cf9cc487480a.pdf.txt\n",
      "Processing file 7: 000cc27b-14a3-4ae6-afb0-6e926c10dc49.pdf.txt\n",
      "Processing file 8: 00105eac-ffd9-40c7-be76-08a86c7ddb9c.pdf.txt\n",
      "Processing file 9: 0011166f-e274-42d1-b39d-7a910c3152a7.pdf.txt\n",
      "Processing file 10: 00121815-a318-4ef3-8ad6-25150c1711f4.pdf.txt\n",
      "✅ Party speeches have been saved to the CSV file: party_speeches.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def sanitize_filename(name, max_length=100):\n",
    "    # Remove/replace invalid characters and truncate\n",
    "    name = re.sub(r'[\\\\/:\"*?<>|]', '_', name)\n",
    "    return name[:max_length].strip()\n",
    "\n",
    "# Match speaker and speech (includes 'De voorzitter')\n",
    "pattern = re.compile(\n",
    "    r'(?:'\n",
    "    r'(?:(?:De heer|Mevrouw)\\s+[^:(]+?\\s+\\((?P<party>[^)]+)\\):)'     # MPs\n",
    "    r'|(?P<voorzitter>De voorzitter):'                                # Chair\n",
    "    r'|(?P<minister>Minister\\s+[A-Z][^\\n:]{1,100}):'                  # Minister\n",
    "    r')\\s*'\n",
    "    r'(?P<speech>.*?)(?=(?:De heer|Mevrouw)\\s+[^:(]+?\\s+\\([^)]+\\):|De voorzitter:|Minister\\s+[A-Z][^\\n:]{1,100}:|$)',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary of party -> list of speeches\n",
    "party_speeches = defaultdict(list)\n",
    "\n",
    "# Read and process each file independently\n",
    "txt_folder = \"txt\"\n",
    "file_count = 0\n",
    "for filename in os.listdir(txt_folder):\n",
    "    if file_count >= 10:  # Stop after processing 10 files\n",
    "        break\n",
    "\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(txt_folder, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        print(f\"Processing file {file_count + 1}: {filename}\")\n",
    "        file_count += 1\n",
    "\n",
    "        # Extract speeches from the current file\n",
    "        for match in pattern.finditer(text):\n",
    "            if match.group(\"party\"):\n",
    "                party = match.group(\"party\").strip()\n",
    "            elif match.group(\"voorzitter\"):\n",
    "                party = \"VOORZITTER\"\n",
    "            elif match.group(\"minister\"):\n",
    "                party = match.group(\"minister\").strip()\n",
    "            else:\n",
    "                continue  # Defensive: skip if no speaker found\n",
    "\n",
    "            speech = match.group(\"speech\").strip()\n",
    "\n",
    "            if len(party) > 100 or '\\n' in party or party.lower().startswith(\"de heer\") or len(party.split()) > 10:\n",
    "                print(f\"⚠️ Skipping invalid party label: {party[:60]}...\")\n",
    "                continue\n",
    "\n",
    "            party_speeches[party].append(speech)\n",
    "\n",
    "# Prepare data for CSV\n",
    "data = []\n",
    "for party, speeches in party_speeches.items():\n",
    "    for speech in speeches:\n",
    "        data.append({\"Filename\": filename, \"Party\": party, \"Speech\": speech})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_csv = \"party_speeches.csv\"\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Party speeches have been saved to the CSV file: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d972ea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TypeIs' from 'typing_extensions' (c:\\Users\\Jacco\\anaconda3\\Lib\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer\n",
      "File \u001b[1;32mc:\\Users\\Jacco\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:71\u001b[0m\n\u001b[0;32m     69\u001b[0m         typing_extensions\u001b[38;5;241m.\u001b[39mTypeIs \u001b[38;5;241m=\u001b[39m _TypeIs\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeIs \u001b[38;5;28;01mas\u001b[39;00m _TypeIs\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __version__\n\u001b[0;32m     75\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoolStorage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoolTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m ]\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TypeIs' from 'typing_extensions' (c:\\Users\\Jacco\\anaconda3\\Lib\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"manifesto-project/manifestoberta-xlm-roberta-56policy-topics-sentence-2024-1-1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "# Path to speeches CSV\n",
    "input_csv = \"party_speeches.csv\"\n",
    "output_csv = \"party_speeches_classification.csv\"\n",
    "\n",
    "# Read speeches from CSV\n",
    "df_speeches = pd.read_csv(input_csv)\n",
    "results = []\n",
    "\n",
    "for index, row in df_speeches.iterrows():\n",
    "    party_name = row[\"party\"]\n",
    "    text = row[\"speech\"]\n",
    "\n",
    "    if not text or pd.isna(text):\n",
    "        print(f\"⚠️ Skipping empty speech for party: {party_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{party_name}: Speech has {len(text)} characters\")\n",
    "\n",
    "    # Split into sentences and optionally filter\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(f\"{party_name}: Extracted {len(sentences)} total sentences\")\n",
    "\n",
    "    max_samples = 50  # Limit number of samples for speed\n",
    "    sentences = [s for s in sentences if len(s.split()) > 5][:max_samples]\n",
    "    print(f\"{party_name}: Using {len(sentences)} filtered sentences for classification\")\n",
    "\n",
    "    if not sentences:\n",
    "        print(f\"⚠️ No valid sentences found for party: {party_name}\")\n",
    "        continue\n",
    "\n",
    "    topic_scores = torch.zeros(len(model.config.id2label))\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        inputs = tokenizer(sent,\n",
    "                           return_tensors=\"pt\",\n",
    "                           max_length=200,\n",
    "                           padding=\"max_length\",\n",
    "                           truncation=True)\n",
    "\n",
    "        print(f\"Sentence {i+1}: {len(inputs['input_ids'][0])} tokens\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "        topic_scores += probs\n",
    "\n",
    "    # Average topic scores\n",
    "    topic_scores /= len(sentences)\n",
    "\n",
    "    # Format results\n",
    "    probabilities = {\n",
    "        model.config.id2label[i]: round(score.item() * 100, 2)\n",
    "        for i, score in enumerate(topic_scores)\n",
    "    }\n",
    "    probabilities = dict(sorted(probabilities.items(), key=lambda x: x[1], reverse=True))\n",
    "    predicted_class = max(probabilities, key=probabilities.get)\n",
    "\n",
    "    result = {\n",
    "        \"party\": party_name,\n",
    "        \"predicted_class\": predicted_class,\n",
    "        \"top_3\": list(probabilities.items())[:3]\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    print(f\"{party_name}: {predicted_class}\")\n",
    "    print(f\"Top 3 topics: {result['top_3']}\\n\")\n",
    "\n",
    "# Prepare data for CSV\n",
    "classification_data = []\n",
    "for result in results:\n",
    "    classification_data.append({\n",
    "        \"Party\": result[\"party\"],\n",
    "        \"Predicted Class\": result[\"predicted_class\"],\n",
    "        \"Top 1 Topic\": result[\"top_3\"][0][0],\n",
    "        \"Top 1 Probability (%)\": result[\"top_3\"][0][1],\n",
    "        \"Top 2 Topic\": result[\"top_3\"][1][0] if len(result[\"top_3\"]) > 1 else None,\n",
    "        \"Top 2 Probability (%)\": result[\"top_3\"][1][1] if len(result[\"top_3\"]) > 1 else None,\n",
    "        \"Top 3 Topic\": result[\"top_3\"][2][0] if len(result[\"top_3\"]) > 2 else None,\n",
    "        \"Top 3 Probability (%)\": result[\"top_3\"][2][1] if len(result[\"top_3\"]) > 2 else None,\n",
    "    })\n",
    "\n",
    "# Save results to a CSV file\n",
    "df_classification = pd.DataFrame(classification_data)\n",
    "df_classification.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Classification results saved to: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
