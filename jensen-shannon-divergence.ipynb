{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "df = pd.read_csv('party_speeches_classification_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd76528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no party\n",
    "df = df.dropna(subset=['party'])\n",
    "\n",
    "# Step 1: Build a topic distribution per speech\n",
    "def build_topic_dist(row):\n",
    "    dist = {}\n",
    "    for i in range(1, 4):\n",
    "        topic = row[f'top_{i}_topic']\n",
    "        prob = row[f'top_{i}_prob']\n",
    "        if isinstance(topic, str) and ' - ' in topic:\n",
    "            topic_id = topic.split(' - ')[0].strip()\n",
    "            dist[topic_id] = dist.get(topic_id, 0) + prob\n",
    "    return dist\n",
    "\n",
    "df['topic_dist'] = df.apply(build_topic_dist, axis=1)\n",
    "\n",
    "# Step 2: Aggregate distributions per party\n",
    "party_topic_counts = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    party = row['party']\n",
    "    for topic, prob in row['topic_dist'].items():\n",
    "        party_topic_counts[party][topic] += prob\n",
    "\n",
    "# Get all unique topics\n",
    "all_topics = sorted({topic for counts in party_topic_counts.values() for topic in counts})\n",
    "\n",
    "# Step 3: Normalize to probability distributions\n",
    "party_dists = {}\n",
    "for party, topic_count in party_topic_counts.items():\n",
    "    vec = np.array([topic_count.get(topic, 0) for topic in all_topics])\n",
    "    vec = vec / vec.sum()  # normalize\n",
    "    party_dists[party] = vec\n",
    "\n",
    "# Step 4: Compute pairwise Jensen-Shannon divergence\n",
    "party_names = list(party_dists.keys())\n",
    "jsd_matrix = pd.DataFrame(index=party_names, columns=party_names, dtype=float)\n",
    "\n",
    "for p1, p2 in combinations(party_names, 2):\n",
    "    d1 = party_dists[p1]\n",
    "    d2 = party_dists[p2]\n",
    "    jsd = jensenshannon(d1, d2, base=2)\n",
    "    jsd_matrix.loc[p1, p2] = jsd\n",
    "    jsd_matrix.loc[p2, p1] = jsd\n",
    "\n",
    "np.fill_diagonal(jsd_matrix.values, 0.0)\n",
    "\n",
    "# Optional: Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(jsd_matrix.astype(float), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Jensen-Shannon Divergence Between Parties\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5184a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(jsd_matrix, method='average')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, labels=jsd_matrix.index, orientation='top')\n",
    "plt.title(\"Party Clustering by Topic Distribution (JSD)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ee4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_groups = {\n",
    "    'Economy': {'412', '413', '415'},\n",
    "    'Constitution': {'203', '204'},\n",
    "    'Environment': {'601', '602'},\n",
    "    'Immigration': {'701', '702'},\n",
    "    'Social Policy': {'501', '502'},\n",
    "    # Add more based on your full topic list\n",
    "}\n",
    "\n",
    "\n",
    "def get_issues_for_speech(row, issue_groups):\n",
    "    issues = set()\n",
    "    for i in range(1, 4):\n",
    "        topic = row.get(f'top_{i}_topic', '')\n",
    "        if isinstance(topic, str) and ' - ' in topic:\n",
    "            topic_id = topic.split(' - ')[0].strip()\n",
    "            for issue, topic_ids in issue_groups.items():\n",
    "                if topic_id in topic_ids:\n",
    "                    issues.add(issue)\n",
    "    return list(issues)\n",
    "\n",
    "df['issues'] = df.apply(get_issues_for_speech, axis=1, issue_groups=issue_groups)\n",
    "\n",
    "\n",
    "issue_jsd_results = {}\n",
    "\n",
    "for issue in issue_groups.keys():\n",
    "    subset = df[df['issues'].apply(lambda x: issue in x)]\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    # Aggregate topic distributions per party\n",
    "    party_topic_counts = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for _, row in subset.iterrows():\n",
    "        party = row['party']\n",
    "        if pd.isna(party):\n",
    "            continue\n",
    "        for topic, prob in row['topic_dist'].items():\n",
    "            party_topic_counts[party][topic] += prob\n",
    "\n",
    "    all_topics = sorted({topic for counts in party_topic_counts.values() for topic in counts})\n",
    "\n",
    "    party_dists = {}\n",
    "    for party, topic_count in party_topic_counts.items():\n",
    "        vec = np.array([topic_count.get(topic, 0) for topic in all_topics])\n",
    "        if vec.sum() == 0:\n",
    "            continue\n",
    "        vec = vec / vec.sum()\n",
    "        party_dists[party] = vec\n",
    "\n",
    "    party_names = list(party_dists.keys())\n",
    "    jsd_matrix = pd.DataFrame(index=party_names, columns=party_names, dtype=float)\n",
    "\n",
    "    for p1 in party_names:\n",
    "        for p2 in party_names:\n",
    "            if p1 == p2:\n",
    "                jsd = 0.0\n",
    "            else:\n",
    "                jsd = jensenshannon(party_dists[p1], party_dists[p2], base=2)\n",
    "            jsd_matrix.loc[p1, p2] = jsd\n",
    "\n",
    "    issue_jsd_results[issue] = jsd_matrix\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for issue, matrix in issue_jsd_results.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matrix.astype(float), annot=True, cmap='coolwarm')\n",
    "    plt.title(f\"JSD Between Parties on {issue}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the full JSD matrix (e.g., from earlier)\n",
    "mds = MDS(n_components=1, dissimilarity='precomputed', random_state=42)\n",
    "ideology_scores = mds.fit_transform(jsd_matrix.values)\n",
    "\n",
    "# Attach scores to party names\n",
    "ideology_df = pd.DataFrame({'party': jsd_matrix.index, 'ideology_score': ideology_scores.flatten()})\n",
    "ideology_df = ideology_df.sort_values(by='ideology_score').reset_index(drop=True)\n",
    "print(ideology_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d83b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_leaning = {\n",
    "    'Economy': +1,           # right-leaning\n",
    "    'Constitution': 0,       # neutral\n",
    "    'Environment': -1,       # left\n",
    "    'Immigration': +1,       # right\n",
    "    'Social Policy': -1      # left\n",
    "}\n",
    "\n",
    "\n",
    "party_scores = defaultdict(list)\n",
    "\n",
    "for issue, jsd_mat in issue_jsd_results.items():\n",
    "    if issue not in issue_leaning:\n",
    "        continue\n",
    "    lean = issue_leaning[issue]\n",
    "    avg_dists = jsd_mat.mean(axis=1)\n",
    "    for party, dist in avg_dists.items():\n",
    "        party_scores[party].append(lean * (1 - dist))  # closer = more aligned\n",
    "\n",
    "# Final ideological score per party\n",
    "ideology_scores = {party: np.mean(scores) for party, scores in party_scores.items()}\n",
    "ideology_df = pd.DataFrame.from_dict(ideology_scores, orient='index', columns=['ideology_score']).sort_values(by='ideology_score')\n",
    "print(ideology_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ff885",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_leaning = {\n",
    "    'Economy': +1,           # traditionally right\n",
    "    'Constitution': 0,       # neutral\n",
    "    'Environment': -1,       # traditionally left\n",
    "    'Immigration': +1,       # traditionally right\n",
    "    'Social Policy': -1      # traditionally left\n",
    "}\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "issue_party_scores = {}\n",
    "\n",
    "for issue, topic_ids in issue_groups.items():\n",
    "    subset = df[df['issues'].apply(lambda x: issue in x)]\n",
    "    if subset.empty:\n",
    "        continue\n",
    "\n",
    "    # Aggregate topic distributions per party\n",
    "    party_topic_counts = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for _, row in subset.iterrows():\n",
    "        party = row['party']\n",
    "        if pd.isna(party):\n",
    "            continue\n",
    "        for topic, prob in row['topic_dist'].items():\n",
    "            if topic in topic_ids:\n",
    "                party_topic_counts[party][topic] += prob\n",
    "\n",
    "    # Normalize per party\n",
    "    party_dists = {}\n",
    "    for party, topic_count in party_topic_counts.items():\n",
    "        vec = np.array([topic_count.get(t, 0) for t in topic_ids])\n",
    "        if vec.sum() == 0:\n",
    "            continue\n",
    "        vec = vec / vec.sum()\n",
    "        party_dists[party] = vec\n",
    "\n",
    "    # Compute mean usage for each party (as proxy for alignment)\n",
    "    issue_scores = {}\n",
    "    max_val = 0\n",
    "    for party, vec in party_dists.items():\n",
    "        score = vec.sum()  # total emphasis on this issue\n",
    "        issue_scores[party] = score\n",
    "        max_val = max(max_val, score)\n",
    "\n",
    "    # Normalize & apply ideological direction\n",
    "    if max_val > 0:\n",
    "        for party in issue_scores:\n",
    "            norm_score = issue_scores[party] / max_val\n",
    "            issue_scores[party] = norm_score * issue_leaning.get(issue, 0)\n",
    "\n",
    "    issue_party_scores[issue] = issue_scores\n",
    "\n",
    "\n",
    "\n",
    "# Combine into a DataFrame\n",
    "issue_score_df = pd.DataFrame(issue_party_scores).fillna(0).round(2)\n",
    "issue_score_df = issue_score_df.sort_index()\n",
    "print(issue_score_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which topics belong to which ideological issue\n",
    "issue_groups = {\n",
    "    'Economy': {'412', '413', '415'},\n",
    "    'Constitution': {'203', '204'},\n",
    "    'Environment': {'601', '602'},\n",
    "    'Immigration': {'701', '702'},\n",
    "    'Social Policy': {'501', '502'},\n",
    "}\n",
    "\n",
    "# Define the leaning of each issue\n",
    "issue_leaning = {\n",
    "    'Economy': +1,           # traditionally right\n",
    "    'Constitution': 0,       # neutral\n",
    "    'Environment': -1,       # traditionally left\n",
    "    'Immigration': +1,       # traditionally right\n",
    "    'Social Policy': -1      # traditionally left\n",
    "}\n",
    "\n",
    "# Step 1: Aggregate topic probabilities per party per issue\n",
    "party_issue_strengths = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    party = row['party']\n",
    "    if pd.isna(party):\n",
    "        continue\n",
    "    topic_dist = row['topic_dist']\n",
    "    for topic_id, prob in topic_dist.items():\n",
    "        for issue, issue_topic_ids in issue_groups.items():\n",
    "            if topic_id in issue_topic_ids:\n",
    "                party_issue_strengths[party][issue] += prob\n",
    "\n",
    "# Step 2: Normalize scores per issue (not per party) and apply leaning\n",
    "issue_party_scores = defaultdict(dict)\n",
    "\n",
    "for issue in issue_groups:\n",
    "    # Get raw scores for this issue\n",
    "    raw_scores = {party: party_issue_strengths[party].get(issue, 0.0)\n",
    "                  for party in party_issue_strengths}\n",
    "    max_val = max(raw_scores.values()) if raw_scores else 1.0\n",
    "\n",
    "    for party, score in raw_scores.items():\n",
    "        normalized = score / max_val if max_val > 0 else 0\n",
    "        ideological_score = normalized * issue_leaning.get(issue, 0)\n",
    "        issue_party_scores[party][issue] = ideological_score\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "issue_score_df = pd.DataFrame(issue_party_scores).T.fillna(0).round(2)\n",
    "issue_score_df = issue_score_df.sort_index()\n",
    "print(issue_score_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e75f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon_divergence(p, q):\n",
    "    # Ensure numpy arrays and normalized\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    p = p / p.sum()\n",
    "    q = q / q.sum()\n",
    "    return jensenshannon(p, q)**2  # scipy returns sqrt(JS), square it for divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect topic vectors per party\n",
    "party_topic_vecs = defaultdict(list)\n",
    "\n",
    "# Assuming df['topic_dist'] is dict {topic_id_str: score}, scores sum to 1 per row or close\n",
    "all_topics = set()\n",
    "for _, row in df.iterrows():\n",
    "    party = row.get('party')\n",
    "    topic_dist = row.get('topic_dist')\n",
    "    if pd.isna(party) or not isinstance(topic_dist, dict):\n",
    "        continue\n",
    "    party_topic_vecs[party].append(topic_dist)\n",
    "    all_topics.update(topic_dist.keys())\n",
    "\n",
    "all_topics = sorted(all_topics)  # fix order for vectorization\n",
    "\n",
    "# Average topic distribution per party\n",
    "avg_party_dist = {}\n",
    "for party, dist_list in party_topic_vecs.items():\n",
    "    # Create matrix: rows=documents, columns=topics\n",
    "    mat = np.zeros((len(dist_list), len(all_topics)))\n",
    "    for i, dist in enumerate(dist_list):\n",
    "        for j, topic in enumerate(all_topics):\n",
    "            mat[i, j] = dist.get(topic, 0)\n",
    "    avg = mat.mean(axis=0)\n",
    "    avg /= avg.sum()  # normalize to sum=1\n",
    "    avg_party_dist[party] = avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ace2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = sorted(avg_party_dist.keys())\n",
    "n = len(parties)\n",
    "jsd_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            jsd_matrix[i, j] = 0\n",
    "        elif i < j:\n",
    "            jsd = jensen_shannon_divergence(avg_party_dist[parties[i]], avg_party_dist[parties[j]])\n",
    "            jsd_matrix[i, j] = jsd\n",
    "            jsd_matrix[j, i] = jsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8338bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "from itertools import combinations\n",
    "\n",
    "def mean_jsd_for_coalition(coalition, topic_vectors):\n",
    "    \"\"\"\n",
    "    Compute mean Jensen-Shannon divergence for a set of parties.\n",
    "    Lower means higher thematic alignment.\n",
    "    \"\"\"\n",
    "    if len(coalition) < 2:\n",
    "        return 0.0  # trivial case\n",
    "\n",
    "    jsd_values = []\n",
    "    for p1, p2 in combinations(coalition, 2):\n",
    "        v1 = topic_vectors.get(p1)\n",
    "        v2 = topic_vectors.get(p2)\n",
    "        if v1 is not None and v2 is not None:\n",
    "            jsd = jensenshannon(v1, v2, base=2)\n",
    "            jsd_values.append(jsd)\n",
    "    return np.mean(jsd_values) if jsd_values else 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
